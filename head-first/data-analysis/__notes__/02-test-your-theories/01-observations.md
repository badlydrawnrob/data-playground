## Problems to be solved

Personas -vs- Problem-Personas

> ðŸ’¬ What they say || <mark>TRUTH</mark> || ðŸ‘ What they do

- [ ] Qualitative should be tested with quantitative
    - Perceptions are not always the same as _objective_ behaviour





## Comparing Data

> Always use the <mark>method of comparison</mark> in analysis

![Picture of $sales chunks]()

- [ ] Data is <strong>only</strong> interesting when in <strong>comparison</strong> to other data.
    - <b>November</b> compared to <b>December</b>
    - One statistic compared to other statistics

> Always make comparisons <mark>explicit</mark>

1. It needs to clearly state the <i>assumption</i> about the <i>comparison</i>
2. It needs to communicate to client and yourself

![Picture of brownie observation]()

### Observational Data

![Picture of an eye]()

> Qualitative doesn't tell the whole story, but gathers insights for further experimentation

1. Watch. Observe.
2. See what they do.
3. See what groups they assign themselves to.
4. Take inventory (track observations)

### Flipping the theory

Sometimes you'll have to flip the theory to see if the opposite is going on. A good example of this is <b>value perception</b>:

- Is it a customer sense of value for their money?
    - <q>The price is too high for what you're giving me!</q>
- Or does a customer value something more when the price is higher?
    - <q>The price is too cheap! It mustn't be good quality!</q>

> ðŸ™„ Observational data has it's limitations

- [ ] Be careful you're not drawing the wrong conclusions
    - Take observational data with a pinch of salt


### Say it in pictures

Draw pictures of how you think things relate. This helps you make your ideas explicit and easy to view at-a-glance.

![Brownie sales image]()





## Confounders

> <mark>Always</mark> consider how <mark>confounding</mark> may <mark>affect results</mark>

- [ ] Confounders should make sense in the context of your analysis

![Confounders sketch]()

> ðŸ™„ Sometimes a theory will overlap with observational data. Or, sometimes observations will give rise to a theory.

- [x] If the observational data doesn't describe this theory, you'll need an experiment.
    - [ ] e.g if there's not enough (or good enough) data to validate a theory, or give you predictions â€”Â it's <strong>just a theory</strong>!
    - <q>If I train staff better, will the temperature of the coffee improve?</q>
- [ ] Observational data will often <em>not</em> be able to predict the future





## Experimenting

> You might have two or more theories: <mark>You need an experiment!</mark>

![3 test tubes image]()

> <mark>Always set a baseline</mark>

![Customer segment (theory and control) image]()

- [x] Always set a control group, an A/B test or experiment
    - [ ] You need to know <i>what would have happened</i> without the experiment!
    - [ ] You can't <em>always</em> set a control, but always aim to.





# Randomise

When you randomise, the factors that might otherwise become confounders get equal representation in control/experiment groups.

> <mark>NEEDS EXPANDING / ILLUSTRATION / example</mark>

```text
I think it means â€” if age was a confounder â€” split into separate age groups and test control groups within these micro-groups
```

> <mark>Random</mark> selection gets you as close as possible to <mark>causal relationships</mark>

For example, here's a group of people we want to test an experiment on:

![Image of confounder bug]()

The experiment groups should be random. This equals out any potential <i>confounders</i>, as each should contain them in equal amounts. They are essentially the same, other than the variable you're testing for.

![Image of random experiment]()

- [x] This is the case, <b>even if you don't know what the confounder is!</b>

1. First try to avoid any obvious confounders, like location
    - [ ] See Starbuzz example (grouping by micro-regions)
2. Next, <b>randomly assign</b> those groups to the <b>control</b> and <b>experiment</b>

> This is an example of a <mark>randomised controlled experiment</mark>
